{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from os.path import join\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (159571,)\n",
      "y_train: (159571, 6)\n",
      "X_test: (153164,)\n",
      "y_train: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    }
   ],
   "source": [
    "file_embedding_txt = join('E://DM//NLP//WordVec', 'glove.840B.300d.txt')\n",
    "file_embedding_pickle = join('E://DM//NLP//TMP_MEMORY', 'glove.840B.300d.pickle')\n",
    "labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "train[\"comment_text\"].fillna(\"fillna\")\n",
    "test[\"comment_text\"].fillna(\"fillna\")\n",
    "X_train = train[\"comment_text\"].str.lower()\n",
    "X_test = test[\"comment_text\"].str.lower()\n",
    "y_train = train[labels]\n",
    "\n",
    "print('X_train: ' + str(X_train.shape))\n",
    "print('y_train: ' + str(y_train.shape))\n",
    "print('X_test: ' + str(X_test.shape))\n",
    "print('y_train: ' + str(list(y_train.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196018it [04:51, 7529.72it/s]\n"
     ]
    }
   ],
   "source": [
    "def ReadWord2Vec():\n",
    "    embeddings_index = {}\n",
    "    with open(file_embedding_txt, encoding='utf8') as f:\n",
    "        for line in tqdm(f):\n",
    "            values = line.rstrip().rsplit(' ')\n",
    "            assert len(values)>=300\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "embeddings_index = ReadWord2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    explanation\\nwhy the edits made under my usern...\n",
      "1    d'aww! he matches this background colour i'm s...\n",
      "2    hey man, i'm really not trying to edit war. it...\n",
      "3    \"\\nmore\\ni can't make any real suggestions on ...\n",
      "4    you, sir, are my hero. any chance you remember...\n",
      "Name: comment_text, dtype: object\n",
      "<class 'pandas.core.series.Series'>\n",
      "312735\n",
      "explanation\n",
      "why the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27\n",
      "d'aww! he matches this background colour i'm seemingly stuck with. thanks.  (talk) 21:51, january 11, 2016 (utc)\n",
      "hey man, i'm really not trying to edit war. it's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. he seems to care more about the formatting than the actual info.\n",
      "\"\n",
      "more\n",
      "i can't make any real suggestions on improvement - i wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -i think the references may need tidying so that they are all in the exact same format ie date format etc. i can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\n",
      "\n",
      "there appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up. it's listed in the relevant form eg wikipedia:good_article_nominations#transport  \"\n",
      "you, sir, are my hero. any chance you remember what page that's on?\n"
     ]
    }
   ],
   "source": [
    "# merge is the whole copus merged from training text and test text\n",
    "merge = pd.concat([X_train, X_test]).reset_index(drop=True)\n",
    "print(merge[:5])\n",
    "print(type(merge))\n",
    "print(len(merge))\n",
    "for i in range(5):\n",
    "    print(merge[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get rid of '\\t \\n \\r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    explanation why the edits made under my userna...\n",
      "1    d'aww! he matches this background colour i'm s...\n",
      "2    hey man, i'm really not trying to edit war. it...\n",
      "3    \" more i can't make any real suggestions on im...\n",
      "4    you, sir, are my hero. any chance you remember...\n",
      "Name: comment_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "intab = '\\n\\t\\r'\n",
    "outtab = ''.join([' ' for number in range(len(intab))])\n",
    "trans_dict = str.maketrans(intab, outtab)\n",
    "merge = merge.apply(lambda text:text.translate(trans_dict))\n",
    "print(merge[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get rid of spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################\n",
      "\"    == tip for trolls. ==    1: don't fuck with me.   2: you are gay and will always be gay.    yhbt. yhl.hand.:dloldongsloljewsloldongsyhbt. yhl.hand.:dloljewsyhbt. yhl.hand.:dlolfagsloljewswoploldongsyhbt. yhl.hand.:dloldongsloldongsloldongsloldongsyhbt. yhl.hand.:dloljewsloldongsloldongsloldongsloldongsloldongsloldongsloldongsloldongsloldongsloldongsloljewsloldongsloldongsloldongsloldongsloldongsloldongsloldongs   jesuitx. did.wtc.:d\"\n"
     ]
    }
   ],
   "source": [
    "spam_example = 'dloljewsloldongsloldongsloldongsloldongsloldongsloldongsloldongsloldongsloldongsloldongsloljewsloldongsloldongsloldongsloldongsloldongsloldongsloldongs'\n",
    "spam_mask = merge.str.contains(spam_example, regex=False)\n",
    "spam_example_id = merge[spam_mask].index[0]\n",
    "print('###########################')\n",
    "print(merge[spam_example_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"    == tip for trolls. ==    1: don't fuck with me.   2: you are gay and will always be gay.    yhbt.   yhl.hand.:dloljewsyhbt.         jesuitx. did.wtc.:d\"\n"
     ]
    }
   ],
   "source": [
    "# if the length of a text between two spaces is over MAX_LENGTH, consider it as a spam and remove it\n",
    "MAX_LENGTH_SPAM = 30\n",
    "RemoveSPAM = lambda text: ' '.join([word if len(word)<MAX_LENGTH_SPAM else ' ' for word in text.split(' ')])\n",
    "merge = merge.apply(RemoveSPAM)\n",
    "print(merge[spam_example_id])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Normalization\n",
    "* replace full-width characters with half-width characters. for example, convert 'ｄａｙ' to 'day'. convert '１２３' to '123'\n",
    "\n",
    "```python\n",
    "import unicodedata\n",
    "foo = u'１２３４５６７８９０'\n",
    "unicodedata.normalize('NFKC', foo)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    explanation why the edits made under my userna...\n",
      "1    d'aww! he matches this background colour i'm s...\n",
      "2    hey man, i'm really not trying to edit war. it...\n",
      "3    \" more i can't make any real suggestions on im...\n",
      "4    you, sir, are my hero. any chance you remember...\n",
      "Name: comment_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "Normalize = lambda text:unicodedata.normalize('NFKC', text)\n",
    "merge = merge.apply(Normalize)\n",
    "print(merge[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove IP address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    explanation why the edits made under my userna...\n",
      "1    d'aww! he matches this background colour i'm s...\n",
      "2    hey man, i'm really not trying to edit war. it...\n",
      "3    \" more i can't make any real suggestions on im...\n",
      "4    you, sir, are my hero. any chance you remember...\n",
      "Name: comment_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pattern_ip = r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}'\n",
    "RemoveIP = lambda text: re.sub(pattern_ip, ' ip ', text)\n",
    "merge = merge.apply(RemoveIP)\n",
    "print(merge[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### how to deal with hyperLink\n",
    "\n",
    "* one reason that hyperlinks should not be removed is that, the words in the links may have semantic meanings\n",
    "* another reason is that a regular expression covers all posible situations is too complecated, see [this link](https://stackoverflow.com/questions/161738/what-is-the-best-regular-expression-to-check-if-a-string-is-a-valid-url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove unwanted unicode alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    explanation why the edits made under my userna...\n",
      "1    d'aww! he matches this background colour i'm s...\n",
      "2    hey man, i'm really not trying to edit war. it...\n",
      "3    \" more i can't make any real suggestions on im...\n",
      "4    you, sir, are my hero. any chance you remember...\n",
      "Name: comment_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import regex\n",
    "\n",
    "RemoveNonLatin_a = lambda text: regex.sub(r'[^\\p{Latin}\\p{Punctuation}\\p{Number}\\p{Separator}]', u'', text) \n",
    "RemoveNonLatin_b = lambda text: regex.sub(r'[^\\p{InBasic_Latin}\\p{InLatin-1_Supplement}\\p{Punctuation}\\p{Number}\\p{Separator}]', u'', text) \n",
    "RemoveNonLatin = lambda text: RemoveNonLatin_a(RemoveNonLatin_b(text))\n",
    "merge = merge.apply(RemoveNonLatin)\n",
    "print(merge[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### this implementation also works well with hyper link urls\n",
    "* todo: is that meaningful to explicitly add a hyper link token after a hyper link, for example: 'www.google.com ###hyperlink###'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get rid of apostrophe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    explanation why the edits made under my userna...\n",
      "1    d'aww! he matches this background colour i am ...\n",
      "2    hey man, i am really not trying to edit war. i...\n",
      "3    \" more i cannot make any real suggestions on i...\n",
      "4    you, sir, are my hero. any chance you remember...\n",
      "Name: comment_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "APPO = {\n",
    "\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"i would\",\n",
    "\"i'd\" : \"i had\",\n",
    "\"i'll\" : \"i will\",\n",
    "\"i'm\" : \"i am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\": \"it will\",\n",
    "\"i've\" : \"i have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\"\n",
    "}\n",
    "def ConvertText(text):\n",
    "    result = [APPO[word] if word in APPO else word for word in text.split(' ')]\n",
    "    return ' '.join(result)\n",
    "merge = merge.apply(ConvertText)\n",
    "print(merge[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get rid of punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    explanation why the edits made under my userna...\n",
      "1    d aww  he matches this background colour i am ...\n",
      "2    hey man  i am really not trying to edit war  i...\n",
      "3      more i cannot make any real suggestions on i...\n",
      "4    you  sir  are my hero  any chance you remember...\n",
      "Name: comment_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "RemovePunctuations = lambda text: regex.sub(r'[\\p{Punctuation}]', u' ', text) \n",
    "merge = merge.apply(RemovePunctuations)\n",
    "print(merge[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  数据集的文本是不是全部在embeddings_index中，如果不全是，哪些不再embeddings_index中的文本是什么，长什么样?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words in the doc 299082\n",
      "number of words in the word2vec file 2196017\n"
     ]
    }
   ],
   "source": [
    "def ComputeSetOfWords(pd_series):\n",
    "    sets = pd_series.apply(lambda text: set(text.split(' ')))\n",
    "    result = set([])\n",
    "    result = result.union(*sets)\n",
    "    return result\n",
    "word_set = ComputeSetOfWords(merge)\n",
    "vector_set = set(embeddings_index.keys())\n",
    "print('number of words in the doc {}'.format(len(word_set)))\n",
    "print('number of words in the word2vec file {}'.format(len(vector_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一般认为word2vec中的词汇是比较全的，如果在训练数据集中的某个词没有出现在word2vec的词汇中，那么分析下这个词到底是什么会比较有意义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'gunbirdriver', 'vandd', 'odetts', 'rabinal', 'reallavergne', 'yearssince', 'sozialisticheskuyu', 'resnik', 'rockdiedout', 'docuemnts', 'ameriturd', '5000korean', '١٩٩٣', 'registrirajo', 'pooopieee', 'politikospeak', 'nickct', 'poogwalah', 'reeived', 'upnot', 'obozrenie', 'notochords', 'northumberlandshire', 'disctracting', 'qiyamah', 'dekodiranje', 'doputaju', 'ostalocutanje', 'metalsucks', 'worlview', 'konotacje', 'starfire777', 'tueruwfyurf', 'unblockdude', 'cammatti', 'childlish', 'demmenie', 'nüfusu', 'samavedic', 'asgher', 'tål', 'combattive', 'borralo', 'fu9k', 'consideirng', 'channabasappa', 'palono', 'gardenian', 'itsme1234', 'litthe', 'fraunce', 'furme', 'duplom', 'zeitschrift', 'yeahhhhhhhhhhhh', 'fdzvdsf', 'permes', 'faiththat', 'kressenstein', 'staatsanwaltschaft', 'gramostea', 'musephil2006', 'nacionalnoj', 'algonquin7', 'butterpig', 'auckjg', 'choujuu', 'penpad', 'accordigly', 'nornalup', 'helkar', 'senatvs', 'discusseddirect', 'yahawah', 'noev', 'boulez', 'traugott', 'satyameva', 'narutopedia', 'habibie', 'titip', 'instigative', 'innclude', '21387811', 'aschlafly', 'id7820', 'abrirá', 'codelyokofan', '706m', 'forwest', 'mijanmaru', 'cantsee', 'cihuatlán', 'kaikolars', '05066', 'listasmcqueen', 'jonstone', 'kristinia', 'sahay', 'harldly', 'theseems', 'reasns', 'nahoenju', '15denoviembredeldosmildoce', 'chuckalos', 'duckware', 'dnafvdnabxvaahb', 'a930913', 'jpga', 'bloceked', 'letson', 'disrespectong', 'wgee', 'naumi', 'b0lutly', 'siegesshukman', 'calinism', 'protogeometric', 'vandalazying', 'baysamay', 'absgroup', 'plurity', 'verifible', 'propectus', 'sdhould', 'öreg', 'sherpernurnun', 'reliableben', 'zabadani', 'kamph', 'balkano', 'amicis', 'jyotiba', 'sufficien', 'goldschmidt', 'image10', 'allegatios', 'dhssd', 'chuttla', 'dogru144', 'armstong', '9pmet', 'pysics', 'refsespecially', 'thargororlando', 'wheatcroft', 'sbuburb', 'pendelverhalten', 'români', 'sd0001', 'uninational', 'prostitutionlondon', '02west', 'tldefaultsort', 'motherfcuker', 'fdgdf', 'teom', 'ruttledge', 'bobobobobo', 'spitholes', 'chulahoma', 'bolonys', 'ungemütlich', 'mosyley', 'religiousfreedomwatch', 'moglucy', 'ghorban', 'wkipedia', 'dissmising', 'valleyvale', 'rervert', 'ninpuu', 'misena', 'siriram', 'bucharian', 'adoptism', 'gallio', 'medcab', 'administratore', '195o', 'deiva', 'mak050389', 'evdiance', 'abrax5', 'monbiots', 'ksitom', 'kobye', 'orosius', 'soccy', 'wolftengu', 'latinske', 'srecu', 'richview', 'aov2', 'governmentone', 'cohhed', 'gorenje', 'yxcg', 'anderssons', 'jmw0000', 'valckenier', 'leuchtende', 'julianfuckton', 'pjoef', 'chhina', 'otherthey', 'wpbio', 'youuhaxx0r', 'teachihng', 'guillot', 'enok', 'ithriyah', 'ludim', 'firewalks', 'lattoufedisco', 'pageth', 'herethis', 'csenthilmurugan', 'fakhavar', 'mihály', 'themit', 'insomesia', 'ripleman', 'limburgish', 'suheadings', 'bakhg', 'trudom', 'darksaber14', 'lovric', 'phiip', 'besre', 'yjc', 'addersses', 'yaparim', 'arsip', 'ayh', 'wikiorganization', 'innerdovat', 'hatice', 'beeeech', 'vaginabot', 'alfvén', 'sohambanerjee1998', 'tabmr', 'greetingshhh', 'klusx', 'mecico', 'kuptoj', 'hipocritics', 'contervisy', 'argubably', 'unilatral', 'sistematic', 'tzvi', '28686', 'startribune', 'edberg', 'expolded', 'medellín', 'bleiler', 'devostated', 'talkpages', 'westerholdt', 'riverlife', 'inadverdently', 'ououieyrowerf', 'outlinelist', 'ihantala', 'louseing', 'chrobrego', 'orgueuil', 'morenz', 'votestalking', 'freestylefrappe', 'genatalia', 'pichichi', 'homeoapths', 'kakazai', 'staistic', 'kirilovi', 'standardizirani', 'carlebach', 'bdmoittry', 'bmurray', 'margita', 'vyshyvanyi', 'cigaretteswhy', 'anathallo', 'ijustreadbooks', 'rgbnfwdnfcvlubsgiwtgek', 'aksdvj', 'ninly', 'drmargi', 'disquallified', 'otou', 'cononley', 'rediret', 'danshoryu', 'zwack', 'wharrere', 'warrento', 'editno', 'copyrgiht', 'nianta', 'erfolgen', 'iceboat2', 'vaishnavism', 'päijänne', 'nascarking', 'mustanggt5000', 'baristarim', 'bunjevaki', 'judentum', '292370021', 'laaxinna', 'román', 'logdelhi', 'priebe', 'miraluka', 'natioanlity', 'veáse', 'brentcorriganxxx', 'reubuzz', 'ukrajinskoj', 'rageouts', '2001spie', 'vanamonde93', 'geometrc', 'witchelny', 'paone', 'themattsky', 'öztopçu', 'squealingpigattacksagain', 'povu', 'extreemist', 'wysig', 'preußen', 'becklopte', 'dialektism', 'regreattably', 'tetora', 'fxxcking', 'alattyán', 'editirial', 'vandle', 'mountainereers', 'kuvvetlerinin', 'bekämpfung', 'comustion', 'zeldarulah', 'foxlife', 'legitatmate', 'wudl', 'usercheck', 'srihari', 'angenetter', 'funion', 'hoaxipedia', 'aircrats', 'ceranthor', 'prawej', 'kguirnela', 'disregaring', 'ovek', 'amotz', 'auken', 'ballybeg', 'geutanyo', 'warenhaus', 'jednostavnu', 'müjde', 'fishal', '9ow', 'conspiraloons', 'amtorian', 'killr', '6thirteen', 'knaufias', 'tezero', 'quitehelpful', '57613378', 'coscia', 'minthreadsleft10', 'listasking', 'wascowiki', 'jlover', 'paedofile', 'rathead', 'debateget', 'jessifuller', 'moldovanian', 'zwtt', 'borcherding', 'huangshizhai', 'scolopax', 'nswrl', 'heimans', 'sadass', '841m', 'wygrywa', 'vazov', 'emthem', 'acpbg', 'compwhizii', 'stereotyoed', 'brbai', 'tadeusz', 'mccroskey', 'phabdomyolysis', 'zeineddine', 'labberdoodles', 'pancratium', 'relativitätstheorie', 'exhilway', 'navta', 'uncivic', 'rolak', 'grga', 'khwarezmian', 'anteil', 'becaçuse', 'birbirlerini', 'biodanza', 'marsdon', 'avidence', 'nintenfreak', 'complotist', 'forlang', 'kemalrob', 'pookzta', 'dynamy', 'buecker', 'eingliederung', 'kostov', 'f89', 'hammerkop', 'vbdotnetgirl', 'hvorefter', 'lekhug0', 'bosanskoga', 'matassa', 'justise', 'coindesk', '25d1', 'skyring', 'rodinu', 'nijhoff', 'palestinrememberd', 'cma137a', 'spnayol', 'wolfpuss', 'drakken', 'aabbdd', 'peberholm', 'disbute', 'konanim', '1982que', '1213213', 'overeccerod', 'binhi', 'ssb4', 'battleofhaengju', 'kosofsky', 'avval', 'trure', 'propposal', 'jyut', 'ocndusive', 'sungular', 'tudela', 'lithman', 'jeschet', 'singletonb', 'ursäktar', 'multidimensinal', 'trackinfo', '3634954240', 'wikientity', 'lindenbaum', 'polyiandry', 'kürtlerinin', 'dobije', 'tiawanaku', 'skrzyniarz', 'lermontov', 'duelistcard', 'depu', 'kerunocopia', '87290', 'választottad', 'aiojf', '2010digital', 'pentarchy', 'youactually', 'bcm5352', 'mérens', 'mammalsneilsen', 'plagens', 'vedike', 'clientson', 'execution2', 'santhoshriha', 'mysideae', 'teisterbant', 'empyrium', 'tvému', 'komumonisme', 'yeagh', 'spesialist', 'jubnip', 'haphar', 'lespès', 'dolibarr', 'ãã', 'namaqua', 'rowspan31', 'bohun', 'comukmarfor', 'anpther', 'kosawlski', 'kurukkal', 'starnberg', 'kindeness', 'marijuanaisbad', 'földszintjébe', 'trooled', 'bergelson', '04rg9tm', 'isaaq', 'karaorevo', 'decike', 'tab37404', 'contribbs', 'anglocentrism', 'kudarat', '292369952', 'dinamon', 'rmcd', 'eurythmists', 'gurrillas', 'lanwi1', 'requestedarchitectureofthe', 'fjkdcndejsf', 'verfehlungen', 'parlatecontribs', 'lyingly', 'bourland', 'scratchensniff', 'mmoore', 'cover07pelvislg', 'possony', 'atrus', 'annilie', 'anama', 'izvukli', 'khande', 'oneclickarchiver', 'pijaom', 'commenerating', 't650', 'libsey', 'amucalar', 'arielhumble', 'løset', 'zeising1', 'indianhilbilly', 'jayjoanz', 'índios', 'skon', 'obective', 'detmold', 'encylodedia', 'sacrifced', 'mymouth', 'sprecken', 'emprial', 'logsdive', 'plebgate', 'condraticts', 'schiffmann', 'humanpower', 'dntp', 'kramden', 'nueronal', 'klryohtkho', 'liepitz', 'racionalnog', 'poklapaju', 'osxfaq', 'mensea', 'isael', 'udayar', 'meghaninmotion', 'dronrijp', 'amawe', 'barreiro', 'choby', 'hsdvcnjsd', 'escstudent774441', 'ricardio', 'chidlow', 'kosoven', '12bpp', 'kinneret', 'stupididty', 'rituparna', 'gnetum', 'puellanivis', 'recropped', 'lanari', 'obi2canib', '2012malvinas', 'schemozzle', 'nyilván', 'lawsute', 'verbraucht', 'manteit', 'seny', 'xarafka', 'ublicize', 'svelto', 'helendewitt', 'mathematicion', 'kwns', 'ghdfg', 'infinity88', 'omvedt', 'sesplan', 'brandnewz', 'kwamikagam', 'jalaram', 'queerbag', 'hallenbeck', 'osin', 'setuping', 'yabrudian', 'wikiscanner', 'warrish', 'mesrobian', 'unilatery', '004435', 'sidies', 'liron', 'poels', 'disgreed', 'barnstarred', 'multidate8', 'nonlipogram', 'barbariouse', 'pontygwaith', 'juidgement', 'nekokoneko', 'ulküman', 'borts', 'fulcher', 'panjabipunjab', 'duesenburgs', 'annarr', 'evasio', 'bh16010', 'vietiniai', 'dongmyeong', 'kalik', 'thírd', 'yermolov', 'czonków', 'hgyoku', 'menton576', 'abuslute', 'kemitic', 'johnbibby', 'gibnews', 'yakutia', 'navode', 'mgodwin', 'malfuncton', 'roesler', 'nimapara', 'reltively', 'kudzu1', 'sarapeum', 'demnal', 'specificaion', 'davationof', 'ænës', 'poindexters', 'rimar', 'oprahwasontv', 'theoryarthur', 'ghislaine', 'systematis', 'ahmadinejads', 'wikimath', 'furtue', 'matsusaka', 'kjelsås', 'karakalpaks', 'fzjfzjfzjfgjfgzjfgj', 'shnirelman', 'florejacs', '362617', 'frumor', 'muhaiyyaddeen', 'catholicculture', 'clanaka', 'about16', '2188315593', 'rimljanskh', 'vhu', 'coofficial', 'atrian', 'renotify', 'molio', 'hahahaheeeahhhhhhh', 'vorwiegend', 'obrie', 'otysz', 'xsmg', 'zavrsio', 'titlephilosophical', 'tieghem', 'vanalism', 'theissues', 'neate', 'qucky', 'faleg', 'videographies', 'concenscus', 'supratau', 'montecristi', 'chrisnoscrub047', 'leijden', 'riujregheuith', '13we', 'intaa', 'arpajon', 'milans', 'yfhruwg', 'dvoira', 'andreas2009', 'yawljames', 'pölsa', 'naveaps', 'mettoy', 'autoblocks', 'ilald', 'struten', 'yakini', 'meaningimposed', 'seanduc11', 'ghedd', 'hamgyong', 'ricdod', 'ndera', 'madhavpur', 'giuba', 'helenabella', 'contributuons', 'unl337', 'incohorent', '617741', 'niqs', 'whatesover', 'sbvr', 'khost', 'theshiznit', 'damshaq', 'impala99', 'rupestres', 'kinsatzgruppen', 'kambhoj', 'cocolacoste', 'noooob', 'satifiable', 'trynim', 'jiray', 'gzft', 'godiste', 'scieno', 'winafreeipod', 'uguuuu', 'noiscore', 'exop', 'blpo', 'naerie', 'rfcbio', 'hrvacki', 'opte', 'kolis', 'nemonic', 'abortionref2007', 'date2004', 'computings', 'tytyty', 'suchest', 'norden1990', 'telisinda', 'yidsbury', 'apisandbox']\n"
     ]
    }
   ],
   "source": [
    "diff = word_set.difference(vector_set)\n",
    "print(len(diff))\n",
    "show = list(diff)\n",
    "if len(show)>800: show=show[:800]\n",
    "print(show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### other text preprocessing\n",
    "it would be better if the following transformation can be performed:\n",
    "- 'pic005' ->  'pic 005'.\n",
    "- 'seba1978' ->  'seba 1978'\n",
    "- '23help' ->  '23 help'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSeperateWordNumber = lambda text: regex.sub(r'([\\\\p{Latin}]+)([\\\\p{Number}]+)', r'\\\\g<1> \\\\g<2>', text)\\nSeperateNumberWord = lambda text: regex.sub(r'([\\\\p{Number}]+)([\\\\p{Latin}]+)', r'\\\\g<1> \\\\g<2>', text)\\nmerge = merge.apply(SeperateWordNumber)\\nmerge = merge.apply(SeperateNumberWord)\\nprint(merge[:5])\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "SeperateWordNumber = lambda text: regex.sub(r'([\\p{Latin}]+)([\\p{Number}]+)', r'\\g<1> \\g<2>', text)\n",
    "SeperateNumberWord = lambda text: regex.sub(r'([\\p{Number}]+)([\\p{Latin}]+)', r'\\g<1> \\g<2>', text)\n",
    "merge = merge.apply(SeperateWordNumber)\n",
    "merge = merge.apply(SeperateNumberWord)\n",
    "print(merge[:5])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get rid of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    explanation     edits made     username hardco...\n",
      "1    d aww    matches   background colour i   seemi...\n",
      "2    hey man  i   really   trying   edit war       ...\n",
      "3        i cannot make   real suggestions   improve...\n",
      "4       sir      hero    chance   remember   page  ...\n",
      "Name: comment_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = stopwords.words(\"english\")\n",
    "eng_stopwords = [word for word in eng_stopwords if len(word)>1]\n",
    "eng_stopwords = set(eng_stopwords)\n",
    "RemoveStopWords = lambda text: ' '.join([word if word not in eng_stopwords else ' ' for word in text.split(' ')])\n",
    "merge = merge.apply(RemoveStopWords)\n",
    "print(merge[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get rid of  long words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    explanation     edits made     username hardco...\n",
      "1    d aww    matches   background colour i   seemi...\n",
      "2    hey man  i   really   trying   edit war       ...\n",
      "3        i cannot make   real suggestions   improve...\n",
      "4       sir      hero    chance   remember   page  ...\n",
      "Name: comment_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGH = 20\n",
    "RemoveLongWords = lambda text: ' '.join([word if len(word)<=MAX_LENGH else ' ' for word in text.split(' ')]) \n",
    "merge = merge.apply(RemoveLongWords)\n",
    "print(merge[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get rid of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    explanation     edits made     username hardco...\n",
      "1    d aww    matches   background colour i   seemi...\n",
      "2    hey man  i   really   trying   edit war       ...\n",
      "3        i cannot make   real suggestions   improve...\n",
      "4       sir      hero    chance   remember   page  ...\n",
      "Name: comment_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "RemoveNumber = lambda text: ' '.join([word if not word.isdigit() else ' ' for word in text.split(' ')])\n",
    "merge = merge.apply(RemoveNumber)\n",
    "print(merge[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'gunbirdriver', 'vandd', 'odetts', 'rabinal', 'reallavergne', 'yearssince', 'sozialisticheskuyu', 'resnik', 'rockdiedout', 'docuemnts', 'ameriturd', '5000korean', 'registrirajo', 'pooopieee', 'politikospeak', 'nickct', 'poogwalah', 'reeived', 'upnot', 'obozrenie', 'notochords', 'northumberlandshire', 'disctracting', 'qiyamah', 'dekodiranje', 'doputaju', 'ostalocutanje', 'metalsucks', 'worlview', 'konotacje', 'starfire777', 'tueruwfyurf', 'unblockdude', 'cammatti', 'childlish', 'demmenie', 'nüfusu', 'samavedic', 'asgher', 'tål', 'combattive', 'borralo', 'fu9k', 'consideirng', 'channabasappa', 'palono', 'gardenian', 'itsme1234', 'litthe', 'fraunce', 'furme', 'duplom', 'zeitschrift', 'yeahhhhhhhhhhhh', 'fdzvdsf', 'permes', 'faiththat', 'kressenstein', 'staatsanwaltschaft', 'gramostea', 'musephil2006', 'nacionalnoj', 'algonquin7', 'butterpig', 'auckjg', 'choujuu', 'penpad', 'accordigly', 'nornalup', 'helkar', 'senatvs', 'discusseddirect', 'yahawah', 'noev', 'boulez', 'traugott', 'satyameva', 'narutopedia', 'habibie', 'titip', 'instigative', 'innclude', '706m', 'aschlafly', 'id7820', 'abrirá', 'codelyokofan', 'forwest', 'mijanmaru', 'cantsee', 'cihuatlán', 'kaikolars', 'listasmcqueen', 'jonstone', 'kristinia', 'sahay', 'harldly', 'theseems', 'reasns', 'nahoenju', 'chuckalos', 'duckware', 'dnafvdnabxvaahb', 'a930913', 'jpga', 'bloceked', 'letson', 'disrespectong', 'wgee', 'naumi', 'b0lutly', 'siegesshukman', 'calinism', 'protogeometric', 'vandalazying', 'baysamay', 'absgroup', 'plurity', 'verifible', 'propectus', 'sdhould', 'öreg', 'sherpernurnun', 'reliableben', 'zabadani', 'kamph', 'balkano', 'amicis', 'jyotiba', 'sufficien', 'goldschmidt', 'image10', 'allegatios', 'dhssd', 'chuttla', 'dogru144', 'armstong', '9pmet', 'pysics', 'refsespecially', 'thargororlando', 'wheatcroft', 'sbuburb', 'pendelverhalten', 'români', 'sd0001', 'uninational', 'prostitutionlondon', '02west', 'tldefaultsort', 'motherfcuker', 'fdgdf', 'teom', 'ruttledge', 'bobobobobo', 'spitholes', 'chulahoma', 'bolonys', 'ungemütlich', 'mosyley', 'moglucy', 'ghorban', 'wkipedia', 'dissmising', 'valleyvale', 'rervert', 'ninpuu', 'misena', 'siriram', 'bucharian', 'adoptism', 'gallio', 'medcab', 'administratore', '195o', 'deiva', 'mak050389', 'evdiance', 'abrax5', 'monbiots', 'ksitom', 'kobye', 'orosius', 'soccy', 'wolftengu', 'latinske', 'srecu', 'richview', 'aov2', 'governmentone', 'cohhed', 'gorenje', 'yxcg', 'anderssons', 'jmw0000', 'valckenier', 'leuchtende', 'julianfuckton', 'pjoef', 'chhina', 'otherthey', 'wpbio', 'youuhaxx0r', 'teachihng', 'guillot', 'enok', 'ithriyah', 'ludim', 'firewalks', 'lattoufedisco', 'pageth', 'herethis', 'csenthilmurugan', 'fakhavar', 'mihály', 'themit', 'insomesia', 'ripleman', 'limburgish', 'suheadings', 'bakhg', 'trudom', 'darksaber14', 'lovric', 'phiip', 'besre', 'yjc', 'addersses', 'yaparim', 'arsip', 'ayh', 'wikiorganization', 'innerdovat', 'hatice', 'beeeech', 'vaginabot', 'alfvén', 'sohambanerjee1998', 'tabmr', 'greetingshhh', 'klusx', 'mecico', 'kuptoj', 'hipocritics', 'contervisy', 'argubably', 'unilatral', 'sistematic', 'tzvi', 'startribune', 'edberg', 'expolded', 'medellín', 'bleiler', 'devostated', 'talkpages', 'westerholdt', 'ihantala', 'inadverdently', 'ououieyrowerf', 'riverlife', 'outlinelist', 'louseing', 'chrobrego', 'orgueuil', 'morenz', 'votestalking', 'freestylefrappe', 'genatalia', 'pichichi', 'homeoapths', 'kakazai', 'staistic', 'kirilovi', 'standardizirani', 'carlebach', 'bdmoittry', 'bmurray', 'margita', 'vyshyvanyi', 'cigaretteswhy', 'anathallo', 'ijustreadbooks', 'aksdvj', 'ninly', 'drmargi', 'disquallified', 'otou', 'cononley', 'rediret', 'danshoryu', 'zwack', 'wharrere', 'warrento', 'editno', 'copyrgiht', 'nianta', 'erfolgen', 'iceboat2', 'vaishnavism', 'päijänne', 'nascarking', 'mustanggt5000', 'baristarim', 'bunjevaki', 'judentum', 'laaxinna', 'román', 'logdelhi', 'priebe', 'miraluka', 'natioanlity', 'veáse', 'brentcorriganxxx', 'reubuzz', 'ukrajinskoj', 'rageouts', '2001spie', 'vanamonde93', 'geometrc', 'witchelny', 'paone', 'themattsky', 'öztopçu', 'povu', 'extreemist', 'wysig', 'preußen', 'becklopte', 'dialektism', 'regreattably', 'tetora', 'fxxcking', 'alattyán', 'editirial', 'vandle', 'mountainereers', 'kuvvetlerinin', 'bekämpfung', 'comustion', 'zeldarulah', 'foxlife', 'legitatmate', 'wudl', 'usercheck', 'srihari', 'angenetter', 'funion', 'hoaxipedia', 'aircrats', 'ceranthor', 'prawej', 'kguirnela', 'disregaring', 'ovek', 'amotz', 'auken', 'ballybeg', 'geutanyo', 'warenhaus', 'jednostavnu', 'müjde', 'fishal', '9ow', 'conspiraloons', 'amtorian', 'killr', '6thirteen', 'knaufias', 'tezero', 'quitehelpful', 'coscia', 'minthreadsleft10', 'listasking', 'wascowiki', 'jlover', 'paedofile', 'rathead', 'debateget', 'jessifuller', 'moldovanian', 'zwtt', 'borcherding', 'huangshizhai', 'scolopax', 'nswrl', 'heimans', 'sadass', '841m', 'wygrywa', 'vazov', 'emthem', 'acpbg', 'compwhizii', 'stereotyoed', 'brbai', 'tadeusz', 'mccroskey', 'phabdomyolysis', 'zeineddine', 'labberdoodles', 'pancratium', 'relativitätstheorie', 'exhilway', 'navta', 'uncivic', 'rolak', 'grga', 'khwarezmian', 'anteil', 'becaçuse', 'birbirlerini', 'biodanza', 'marsdon', 'avidence', 'nintenfreak', 'complotist', 'forlang', 'kemalrob', 'pookzta', 'dynamy', 'buecker', 'eingliederung', 'kostov', 'f89', 'hammerkop', 'vbdotnetgirl', 'hvorefter', 'lekhug0', 'bosanskoga', 'matassa', 'justise', 'coindesk', '25d1', 'skyring', 'rodinu', 'nijhoff', 'palestinrememberd', 'cma137a', 'spnayol', 'wolfpuss', 'drakken', 'aabbdd', 'peberholm', 'disbute', 'konanim', '1982que', 'overeccerod', 'binhi', 'ssb4', 'battleofhaengju', 'kosofsky', 'avval', 'trure', 'propposal', 'jyut', 'ocndusive', 'sungular', 'tudela', 'lithman', 'jeschet', 'singletonb', 'ursäktar', 'multidimensinal', 'trackinfo', 'wikientity', 'lindenbaum', 'polyiandry', 'kürtlerinin', 'dobije', 'tiawanaku', 'skrzyniarz', 'lermontov', 'duelistcard', 'depu', 'kerunocopia', 'választottad', 'aiojf', '2010digital', 'pentarchy', 'youactually', 'bcm5352', 'mérens', 'mammalsneilsen', 'plagens', 'vedike', 'clientson', 'execution2', 'santhoshriha', 'mysideae', 'teisterbant', 'empyrium', 'tvému', 'komumonisme', 'yeagh', 'spesialist', 'jubnip', 'haphar', 'lespès', 'dolibarr', 'ãã', 'namaqua', 'rowspan31', 'bohun', 'comukmarfor', 'anpther', 'kosawlski', 'kurukkal', 'starnberg', 'kindeness', 'marijuanaisbad', 'földszintjébe', 'trooled', 'bergelson', '04rg9tm', 'isaaq', 'karaorevo', 'decike', 'tab37404', 'contribbs', 'anglocentrism', 'kudarat', 'eurythmists', 'dinamon', 'rmcd', 'gurrillas', 'lanwi1', 'fjkdcndejsf', 'verfehlungen', 'parlatecontribs', 'lyingly', 'bourland', 'scratchensniff', 'mmoore', 'cover07pelvislg', 'possony', 'atrus', 'annilie', 'anama', 'izvukli', 'khande', 'oneclickarchiver', 'pijaom', 'commenerating', 't650', 'libsey', 'amucalar', 'arielhumble', 'løset', 'zeising1', 'indianhilbilly', 'jayjoanz', 'índios', 'skon', 'obective', 'detmold', 'encylodedia', 'sacrifced', 'mymouth', 'sprecken', 'emprial', 'logsdive', 'plebgate', 'condraticts', 'schiffmann', 'humanpower', 'dntp', 'kramden', 'nueronal', 'klryohtkho', 'liepitz', 'racionalnog', 'poklapaju', 'osxfaq', 'mensea', 'isael', 'udayar', 'meghaninmotion', 'dronrijp', 'amawe', 'barreiro', 'choby', 'hsdvcnjsd', 'escstudent774441', 'ricardio', 'chidlow', 'kosoven', '12bpp', 'kinneret', 'stupididty', 'rituparna', 'gnetum', 'puellanivis', 'recropped', 'lanari', 'obi2canib', '2012malvinas', 'schemozzle', 'nyilván', 'lawsute', 'verbraucht', 'manteit', 'seny', 'xarafka', 'ublicize', 'svelto', 'helendewitt', 'mathematicion', 'kwns', 'ghdfg', 'infinity88', 'omvedt', 'sesplan', 'brandnewz', 'kwamikagam', 'jalaram', 'queerbag', 'hallenbeck', 'osin', 'setuping', 'yabrudian', 'wikiscanner', 'warrish', 'mesrobian', 'unilatery', 'sidies', 'liron', 'poels', 'disgreed', 'barnstarred', 'multidate8', 'nonlipogram', 'barbariouse', 'pontygwaith', 'juidgement', 'nekokoneko', 'ulküman', 'borts', 'fulcher', 'panjabipunjab', 'duesenburgs', 'annarr', 'evasio', 'bh16010', 'vietiniai', 'dongmyeong', 'kalik', 'thírd', 'yermolov', 'czonków', 'hgyoku', 'menton576', 'abuslute', 'kemitic', 'johnbibby', 'gibnews', 'yakutia', 'navode', 'mgodwin', 'malfuncton', 'roesler', 'nimapara', 'reltively', 'kudzu1', 'sarapeum', 'demnal', 'specificaion', 'davationof', 'ænës', 'poindexters', 'rimar', 'oprahwasontv', 'theoryarthur', 'ghislaine', 'systematis', 'ahmadinejads', 'wikimath', 'furtue', 'matsusaka', 'kjelsås', 'karakalpaks', 'fzjfzjfzjfgjfgzjfgj', 'shnirelman', 'florejacs', 'frumor', 'muhaiyyaddeen', 'catholicculture', 'clanaka', 'about16', 'rimljanskh', 'vhu', 'coofficial', 'atrian', 'renotify', 'molio', 'hahahaheeeahhhhhhh', 'vorwiegend', 'obrie', 'otysz', 'xsmg', 'zavrsio', 'titlephilosophical', 'tieghem', 'vanalism', 'theissues', 'neate', 'qucky', 'faleg', 'videographies', 'concenscus', 'supratau', 'montecristi', 'chrisnoscrub047', 'leijden', 'riujregheuith', '13we', 'intaa', 'arpajon', 'milans', 'yfhruwg', 'dvoira', 'andreas2009', 'yawljames', 'pölsa', 'naveaps', 'mettoy', 'autoblocks', 'ilald', 'struten', 'yakini', 'meaningimposed', 'seanduc11', 'ghedd', 'hamgyong', 'ricdod', 'ndera', 'madhavpur', 'giuba', 'helenabella', 'contributuons', 'unl337', 'incohorent', 'niqs', 'whatesover', 'sbvr', 'khost', 'theshiznit', 'damshaq', 'impala99', 'rupestres', 'kinsatzgruppen', 'kambhoj', 'cocolacoste', 'noooob', 'satifiable', 'trynim', 'jiray', 'gzft', 'godiste', 'scieno', 'winafreeipod', 'uguuuu', 'noiscore', 'exop', 'blpo', 'naerie', 'rfcbio', 'hrvacki', 'opte', 'kolis', 'nemonic', 'abortionref2007', 'date2004', 'computings', 'tytyty', 'suchest', 'norden1990', 'telisinda', 'yidsbury', 'apisandbox', 'g0re', 't0mpr1c3', 'k11c', 'kiwiexile', 'krayiot', 'neubius', 'brobablt', 'skanger', 'kazutoshi', 'exfociont', 'questining', 'damdama', 'occupators', 'kommunisten', 'empor', 'importantley', 'bloomaisha', 'dorost', 'sarcosuchus']\n"
     ]
    }
   ],
   "source": [
    "word_set = ComputeSetOfWords(merge)\n",
    "diff = word_set.difference(vector_set)\n",
    "print(len(diff))\n",
    "show = list(diff)\n",
    "if len(show)>800: show=show[:800]\n",
    "print(show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### combine whitespaces\n",
    "strange chars were replaced with whitespaces in the previous steps. we need to combine multiple whitespaces into one whitespace. for example 'The   fox jumped   over    the log' -> 'The fox jumped over the log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    explanation edits made username hardcore metal...\n",
      "1    d aww matches background colour i seemingly st...\n",
      "2    hey man i really trying edit war guy constantl...\n",
      "3    i cannot make real suggestions improvement i w...\n",
      "4                        sir hero chance remember page\n",
      "Name: comment_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "RemoveMultipleSpace = lambda text: ' '.join(text.split())\n",
    "merge = merge.apply(RemoveMultipleSpace)\n",
    "print(merge[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 验证数据中的字符集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "〇\n",
      "'〇'\n",
      "12295\n"
     ]
    }
   ],
   "source": [
    "def ComputeSetOfAlphabets(pd_series):\n",
    "    sets = pd_series.apply(lambda text: set(str(text)))\n",
    "    result = set([])\n",
    "    result = result.union(*sets)\n",
    "    return result\n",
    "\n",
    "def ShowUnicodeCodePoint(character):\n",
    "    print(character)\n",
    "    print(repr(character))\n",
    "    print(ord(character))\n",
    "    \n",
    "alphabet_set = ComputeSetOfAlphabets(merge)\n",
    "alphabet = list(alphabet_set)\n",
    "alphabet.sort()\n",
    "# print(alphabet)\n",
    "ShowUnicodeCodePoint(alphabet[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = train.copy()\n",
    "train_clean[\"comment_text\"] = merge[:len(train)]\n",
    "test_clean = test.copy()\n",
    "test_clean[\"comment_text\"] = merge[len(train):].reset_index(drop=True, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'E:\\\\DM\\\\NLP\\\\TMP_MEMORY'\n",
    "path_train_clean = join(root, 'train_clean.csv')\n",
    "path_test_clean = join(root, 'test_clean.csv')\n",
    "train_clean.to_csv(path_train_clean, encoding='utf-8', index=False, header=True)\n",
    "test_clean.to_csv(path_test_clean, encoding='utf-8', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTIONS = []\n",
    "NAMES = []\n",
    "\n",
    "# lower case first\n",
    "FUNCTIONS.append(lambda text:text.lower())\n",
    "NAMES.append('lower case first')\n",
    "\n",
    "# get rid of '\\t \\n \\r'\n",
    "intab = '\\n\\t\\r'\n",
    "outtab = ''.join([' ' for number in range(len(intab))])\n",
    "trans_dict = str.maketrans(intab, outtab)\n",
    "FUNCTIONS.append(lambda text:text.translate(trans_dict))\n",
    "NAMES.append('get rid of \\\\t \\\\n \\\\r')\n",
    "\n",
    "# Get rid of spam\n",
    "MAX_LENGTH_SPAM = 30\n",
    "FUNCTIONS.append(lambda text: ' '.join([word if len(word)<MAX_LENGTH_SPAM else ' ' for word in text.split(' ')]))\n",
    "NAMES.append('Get rid of spam')\n",
    "\n",
    "# Text Normalization\n",
    "import unicodedata\n",
    "FUNCTIONS.append(lambda text:unicodedata.normalize('NFKC', text))\n",
    "NAMES.append('Text Normalization')\n",
    "\n",
    "# remove IP address\n",
    "pattern_ip = r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}'\n",
    "FUNCTIONS.append(lambda text: re.sub(pattern_ip, ' ip ', text))\n",
    "NAMES.append('remove IP address')\n",
    "\n",
    "# remove unwanted unicode alphabet\n",
    "import regex\n",
    "RemoveNonLatin_a = lambda text: regex.sub(r'[^\\p{Latin}\\p{Punctuation}\\p{Number}\\p{Separator}]', u'', text) \n",
    "RemoveNonLatin_b = lambda text: regex.sub(r'[^\\p{InBasic_Latin}\\p{InLatin-1_Supplement}\\p{Punctuation}\\p{Number}\\p{Separator}]', u'', text) \n",
    "FUNCTIONS.append(lambda text: RemoveNonLatin_a(RemoveNonLatin_b(text)))\n",
    "NAMES.append('remove unwanted unicode alphabet')\n",
    "\n",
    "# get rid of apostrophe\n",
    "def ConvertText(text):\n",
    "    result = [APPO[word] if word in APPO else word for word in text.split(' ')]\n",
    "    return ' '.join(result)\n",
    "FUNCTIONS.append(ConvertText)\n",
    "NAMES.append('get rid of apostrophe')\n",
    "\n",
    "# get rid of punctuations\n",
    "FUNCTIONS.append(lambda text: regex.sub(r'[\\p{Punctuation}]', u' ', text) )\n",
    "NAMES.append('get rid of punctuations')\n",
    "\n",
    "# get rid of stop words\n",
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = stopwords.words(\"english\")\n",
    "eng_stopwords = [word for word in eng_stopwords if len(word)>1]\n",
    "eng_stopwords = set(eng_stopwords)\n",
    "FUNCTIONS.append(lambda text: ' '.join([word if word not in eng_stopwords else ' ' for word in text.split(' ')]))\n",
    "NAMES.append('get rid of stop words')\n",
    "\n",
    "# get rid of long words\n",
    "MAX_LENGH = 20\n",
    "FUNCTIONS.append(lambda text: ' '.join([word if len(word)<=MAX_LENGH else ' ' for word in text.split(' ')]))\n",
    "NAMES.append('get rid of long words')\n",
    "\n",
    "# get rid of numbers\n",
    "FUNCTIONS.append(lambda text: ' '.join([word if not word.isdigit() else ' ' for word in text.split(' ')]))\n",
    "NAMES.append('get rid of numbers')\n",
    "\n",
    "# combine whitespaces\n",
    "FUNCTIONS.append(lambda text: ' '.join(text.split()))\n",
    "NAMES.append('combine whitespaces')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "GetLength = lambda text: len(text.split(' '))\n",
    "length = merge.apply(GetLength)\n",
    "length.sort_values(inplace=True, ascending=False)\n",
    "# length_distribute = length.value_counts(sort=True)\n",
    "extream_examples = length[:200]\n",
    "example_ids = []\n",
    "for example in extream_examples:\n",
    "    example_id = length[length==example].index[0]\n",
    "    example_ids.append(example_id)\n",
    "    # print('##############################################')\n",
    "    # print(merge_origin[example_id])\n",
    "    # print('**********************************************')\n",
    "    # print(merge[example_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit\n"
     ]
    }
   ],
   "source": [
    "##16\n",
    "##15\n",
    "origin = pd.concat([X_train, X_test]).reset_index(drop=True)\n",
    "text = origin[example_ids[195]][:150]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit \n",
      "####################\n",
      "lower case first\n",
      "pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit \n",
      "####################\n",
      "get rid of \\t \\n \\r\n",
      "pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit \n",
      "####################\n",
      "Get rid of spam\n",
      "pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit \n",
      "####################\n",
      "Text Normalization\n",
      "pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit \n",
      "####################\n",
      "remove IP address\n",
      "pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit \n",
      "####################\n",
      "remove unwanted unicode alphabet\n",
      "pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit \n",
      "####################\n",
      "get rid of apostrophe\n",
      "pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit \n",
      "####################\n",
      "get rid of punctuations\n",
      "pelican shit willy on wheels pelican shit willy on wheels pelican shit willy on wheels pelican shit \n",
      "####################\n",
      "get rid of stop words\n",
      "pelican shit willy   wheels pelican shit willy   wheels pelican shit willy   wheels pelican shit \n",
      "####################\n",
      "get rid of long words\n",
      "pelican shit willy   wheels pelican shit willy   wheels pelican shit willy   wheels pelican shit \n",
      "####################\n",
      "get rid of numbers\n",
      "pelican shit willy   wheels pelican shit willy   wheels pelican shit willy   wheels pelican shit \n",
      "####################\n",
      "combine whitespaces\n",
      "pelican shit willy wheels pelican shit willy wheels pelican shit willy wheels pelican shit\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "for func, name in zip(FUNCTIONS, NAMES):\n",
    "    print('####################')\n",
    "    print(name)\n",
    "    text = func(text)\n",
    "    print(text)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
