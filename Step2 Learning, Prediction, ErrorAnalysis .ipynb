{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from os.path import join, isfile\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=100000\n",
    "maxlen=150\n",
    "embed_size=300\n",
    "\n",
    "file_embedding_txt = join('E://DM//NLP//WordVec', 'glove.840B.300d.txt')\n",
    "path_tmp = join('E:\\\\DM\\\\NLP\\\\TMP_MEMORY','Toxic_Comment_Classification')\n",
    "file_model = join(path_tmp, 'weights_base.best.hdf5')\n",
    "file_embedding_matrix = join(path_tmp, 'embedding_matrix.hkl')\n",
    "file_train = join(path_tmp, 'train_clean.csv')\n",
    "file_test = join(path_tmp, 'test_clean.csv')\n",
    "file_submission = join(path_tmp, 'submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "def GetData(file_train, file_test):\n",
    "    train = pd.read_csv(file_train, encoding='utf-8')\n",
    "    test = pd.read_csv(file_test, encoding='utf-8')\n",
    "    train[\"comment_text\"].fillna('fillna')\n",
    "    test[\"comment_text\"].fillna('fillna')\n",
    "    X_train = train[\"comment_text\"].apply(str)\n",
    "    X_test = test[\"comment_text\"].apply(str)\n",
    "\n",
    "    y_train = train[labels]\n",
    "    merge = pd.concat([X_train, X_test]).reset_index(drop=True)\n",
    "    merge = merge.astype('str')\n",
    "    return X_train, X_test, y_train, merge\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, merge = GetData(file_train, file_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     159571\n",
       "unique    157881\n",
       "top          nan\n",
       "freq          52\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of X_train_seq:[26, 12, 22, 57, 5, 6, 4, 6, 45, 4]\n",
      "shape of X_train_pad: (159571, 150)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features, lower=True)\n",
    "copus = list(X_train) + list(X_test)\n",
    "copus = [str(x) for x in copus]\n",
    "tokenizer.fit_on_texts(copus)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_pad = sequence.pad_sequences(X_train_seq, maxlen=maxlen)\n",
    "X_test_pad = sequence.pad_sequences(X_test_seq, maxlen=maxlen)\n",
    "\n",
    "length_seq = [len(document) for document in X_train_seq[:10]]\n",
    "print('length of X_train_seq:' + str(length_seq))\n",
    "print('shape of X_train_pad: ' + str(X_train_pad.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196018it [04:29, 8136.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196017\n",
      "291655\n",
      "100000\n",
      "(100000, 300)\n",
      "number of words that are not found in the embedding_vector: 21248\n"
     ]
    }
   ],
   "source": [
    "def ReadWord2Vec():\n",
    "    embeddings_index = {}\n",
    "    with open(file_embedding_txt, encoding='utf8') as f:\n",
    "        for line in tqdm(f):\n",
    "            values = line.rstrip().rsplit(' ')\n",
    "            assert len(values)>=300\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "def CreateEmbeddingMatrix():\n",
    "    embeddings_index = ReadWord2Vec()\n",
    "    word_index = tokenizer.word_index\n",
    "    num_words = min(max_features, len(word_index) + 1)\n",
    "    embedding_matrix = np.zeros((num_words, embed_size))\n",
    "    print(len(embeddings_index))\n",
    "    print(len(word_index))\n",
    "    print(max_features)\n",
    "    print(embedding_matrix.shape)\n",
    "    \n",
    "    num_missed = 0\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            num_missed +=1\n",
    "    print('number of words that are not found in the embedding_vector: ' + str(num_missed))\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to store Large numpy arrays on disk in python\n",
    "* stackoverflow question: https://stackoverflow.com/questions/9619199/best-way-to-preserve-numpy-arrays-on-disk\n",
    "* pickle VS. hdf5: https://shocksolution.com/2010/01/10/storing-large-numpy-arrays-on-disk-python-pickle-vs-hdf5adsf/\n",
    "* hickle github: https://github.com/telegraphic/hickle\n",
    "* HDF is good for storing large numpy arrays\n",
    "* hickle aims to be exactly the same in usage as pickle, but using HDF for data storage\n",
    "\n",
    "```python\n",
    "import os\n",
    "import hickle as hkl\n",
    "import numpy as np\n",
    "    \n",
    "# Create a numpy array of data\n",
    "array_obj = np.ones(32768, dtype='float32')\n",
    "    \n",
    "# Dump to file\n",
    "hkl.dump(array_obj, 'test.hkl', mode='w')\n",
    "    \n",
    "# Dump data, with compression\n",
    "hkl.dump(array_obj, 'test_gzip.hkl', mode='w', compression='gzip')\n",
    "  \n",
    "# Compare filesizes\n",
    "print('uncompressed: %i bytes' % os.path.getsize('test.hkl'))\n",
    "print('compressed:   %i bytes' % os.path.getsize('test_gzip.hkl'))\n",
    "    \n",
    "# Load data\n",
    "array_hkl = hkl.load('test_gzip.hkl')\n",
    "    \n",
    "# Check the two are the same file\n",
    "assert array_hkl.dtype == array_obj.dtype\n",
    "assert np.all((array_hkl, array_obj))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hickle as hkl\n",
    "def GetEmbeddingMatrix():\n",
    "    if isfile(file_embedding_matrix):\n",
    "        embedding_matrix = hkl.load(file_embedding_matrix)\n",
    "        hkl.dump(embedding_matrix, file_embedding_matrix, mode='w')\n",
    "    else:\n",
    "        embedding_matrix = CreateEmbeddingMatrix()\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "# embedding_matrix = GetEmbeddingMatrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 150, 300)     30000000    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 150, 300)     0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 150, 256)     329472      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 148, 64)      49216       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 64)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            774         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 30,379,462\n",
      "Trainable params: 379,462\n",
      "Non-trainable params: 30,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# A blog about LSTM-CNNs:\n",
    "# http://konukoii.com/blog/2018/02/19/twitter-sentiment-analysis-using-combined-lstm-cnn-models/\n",
    "from keras.layers import Dense,Input,Bidirectional,Activation,Conv1D,GRU, Dropout,Embedding\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def GetModel():\n",
    "    sequence_input = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False)(sequence_input)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "    x = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    x = concatenate([avg_pool, max_pool]) \n",
    "    # x = Dense(128, activation='relu')(x)\n",
    "    # x = Dropout(0.1)(x)\n",
    "    preds = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-3),metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = GetModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(X_train_pad, y_train, train_size=0.9, random_state=233)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(file_model, monitor='val_acc', save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
    "ra_val = RocAucEvaluation(validation_data=(X_val, y_val), interval = 1)\n",
    "callbacks_list = [ra_val,checkpoint, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),callbacks = callbacks_list,verbose=1)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting....\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(file_model)\n",
    "print('Predicting....')\n",
    "y_pred = model.predict(X_test_pad,batch_size=1024)\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission[labels] = y_pred\n",
    "submission.to_csv(file_submission, index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the original data\n",
    "path_train_origin = 'data/train.csv'\n",
    "path_test_origin = 'data/test.csv'\n",
    "X_train_origin, X_test_origin, y_train, merge_origin = GetData(path_train_origin, path_test_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train_pad\n",
    "y = y_train\n",
    "\n",
    "y_pred = model.predict(X, batch_size=1024)\n",
    "target_class = 0 # toxic\n",
    "target_result = y_pred[:, target_class]\n",
    "target_gt = y[labels[target_class]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86334765\n"
     ]
    }
   ],
   "source": [
    "def Cost(fpr, tpr):\n",
    "    a = 1-tpr\n",
    "    b = fpr\n",
    "    cost = a + 30*b\n",
    "    return cost\n",
    "\n",
    "# get the threshold that maximize the F1 score\n",
    "def GetBestThreshold(gt, pred):\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(gt, pred)\n",
    "    cost = np.inf\n",
    "    threshold = None\n",
    "    for idx in range(len(thresholds)):\n",
    "        current_cost = Cost(fpr[idx], tpr[idx])\n",
    "        if current_cost < cost:\n",
    "            cost = current_cost\n",
    "            threshold = thresholds[idx]\n",
    "    return threshold, cost\n",
    "\n",
    "threshold, _ = GetBestThreshold(target_gt, target_result)\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9806415376334906\n",
      "6734\n",
      "159571\n",
      "0.9577993495058625\n"
     ]
    }
   ],
   "source": [
    "print(metrics.roc_auc_score(target_gt, target_result))\n",
    "target_pred = target_result>threshold\n",
    "mask = target_gt!=target_pred\n",
    "error = np.sum(mask)\n",
    "total = len(mask)\n",
    "print(error)\n",
    "print(total)\n",
    "print((total-error)/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, merge = GetData(file_train, file_test)\n",
    "misclassified_gt = target_gt[mask]\n",
    "misclassified_x =  X_train[mask]\n",
    "#X_train[:5]\n",
    "Error0 = misclassified_x[misclassified_gt==1] # misclassify toxic as healthy\n",
    "Error1 = misclassified_x[misclassified_gt==0] # misclassify healthy as toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a bisexual like a homosexual a heterosexual defined sexual activity much like a year old boy attracted a girl sexually never sex still straight a person actually sexually attracted aroused sex well opposite sex bisexual\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "A Bisexual, like a homosexual or a heterosexual, is not defined by sexual activity. (Much like a 15 year old boy who is attracted to a girl sexually but has never had sex is still straight). A person who is actually sexually attracted/aroused by the same sex as well as the opposite sex is bisexual.\n",
      "######################################\n",
      "know sex foetus\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "\"know the sex of the foetus\"\"\"\n",
      "######################################\n",
      "reply loser un defines vietnam part southeast asia far i know vietnam part asean used part french indochina laos shit countries anyway culture always influenced sea han chinese proper yangtze han chinese fringe indigenous tribes guangzhou guangxi admit vietnamese a bunch wannabe crap people east asian people i spoken thinks vietnam a integral part sea think backward dirty speak a horrible swearing language matter crap spout wikipedia change way people real world think\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "REPLY ABOVE:\n",
      "That was me, loser. The UN defines Vietnam to be part of Southeast Asia. And far as I know Vietnam is part of ASEAN, and used to be part of French Indochina with Laos and all those shit countries Anyway your culture has always been more influenced by SEA than Han Chinese (as in proper Yangtze Han Chinese, not the fringe indigenous tribes in Guangzhou/Guangxi). \n",
      "\n",
      "Just admit that you vietnamese are all a bunch of wannabe crap people. ALL the east asian people I've spoken to thinks of Vietnam as a very integral part of SEA, and we all think you're backward, dirty and speak in a horrible swearing language. Doesn't matter what crap you spout on Wikipedia, won't change the way people in the real world think.\n",
      "######################################\n",
      "hate topic hate hate a person means like person hate turn love\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "hate is my topic is hate\n",
      "if you hate a person means you like that person. Your hate can turn into your love\n",
      "######################################\n",
      "right express opinion consider hasbara crap right identify hasbara crap\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "NO. You have the right to express an opinion that you consider it to be Hasbara, or crap, or both. You do not have the right to identify it as Hasbara, crap, or both.\n",
      "######################################\n",
      "oh get a life prick ip\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "OH GET A LIFE YOU PRICK 81.144.199.2\n",
      "######################################\n",
      "speaking without knowing i talking ok bout original comment i made everlast s entry constantly vandalized eminem fans written a month ago dumb ass got involved site feud started dilated peoples track i believed verse everlast contributed still jabs taken back forth deserve a synopsis idiotic little comment album sales excessive unnecessary belongs fact everyone knows artists agrees ever got everlast would knock living shit outta eminem came eminem fan site\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Speaking without knowing what I'm talking about? OK, how 'bout this: the original comment I made about Everlast's entry constantly being vandalized by Eminem fans was written a month ago before your dumb ass got involved with the site. The feud started with the Dilated Peoples track. That's why I believed that the verse Everlast contributed should be in there (and still do). The jabs taken back and forth after that deserve a synopsis only. Your idiotic little comment about album sales was excessive and unnecessary. It belongs in there no more than the fact that everyone who knows both artists agrees that if they ever got into it, Everlast would knock the living shit outta Eminem. And THAT came off an Eminem fan site.\n",
      "######################################\n",
      "unblocked last time ok fuck i mean best possible way good luck good night\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Unblocked for the last time\n",
      "Ok, now don't fuck up. I mean that in the best possible way.  Good luck and good night.\n",
      "######################################\n",
      "say gay straight\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "that is to say are you gay or straight?\n",
      "######################################\n",
      "i revert vandalism since admins incompetent anyone worth a shit paid attention spi report s i made pe de chinelo dumbasses would need waste time current ani thread\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "All I do is revert vandalism.  It's all there is to do, since the admins are so incompetent.  If anyone worth a shit paid attention to the SPI report(s) I've made about Pe de Chinelo, you dumbasses wouldn't need to waste time with your current ANI thread.\n",
      "######################################\n",
      "i read quality article reference number states s i died attacks put career hold financial backers pulled obviously a genius also team owner whatever stated newton john name goes a long way motor racing everyone knows read references getting high horse go grab a tube vagisil fuck ip\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "\"\n",
      "I read that quality article, reference number 3 where he states, \"\"it's not because I died in the 9/11 attacks that put my career on hold, it's because my financial backers pulled out\"\" He's obviously a genius. Also the team owner or whatever he was stated the \"\"the newton-john name goes a long way in motor racing as everyone knows who she is\"\" Read your own references before getting on your high horse. Go grab yourself a tube of Vagisil and fuck yourself.90.204.13.4  \"\n",
      "######################################\n",
      "a bad little essay i suck game rather i may suck never know i absolutely refuse play integrity honesty mean far values free civility grade school gossips snitches hall monitors another minor piece evidence bizarro world remains bizarro world\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "\"\n",
      " Not a bad little essay. It's not that I suck at the game. Rather, I may suck but we will never know because I absolutely refuse to play. Integrity and honesty mean far more to me than the values-free \"\"civility\"\" of grade-school gossips, snitches and hall monitors. This is just another minor piece of evidence that bizarro world remains bizarro world.\"\n",
      "######################################\n",
      "lecture someone else sarah talking talking a teacher talks a disobedient child get pedestal spout bizarre lines simply incapable decency adminship a licence talk others contempt fucking insulting i rather deal honest asshole i would really like see around a bunch ass kissing arrogant snakes i see spades project goes hell little bit every day i wonder long articles i written bastardised way talk\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "\"\n",
      "Lecture someone else, Sarah. You are talking down. You're talking down a teacher talks to a disobedient child. Get off the pedestal and don't spout such bizarre lines as \"\"simply incapable of decency\"\". Adminship isn't a licence to talk to others with contempt. It's fucking insulting. \n",
      "\n",
      "I'd rather deal with an honest asshole (which I would really like to see around here) than a bunch of ass-kissing, arrogant snakes (which I see in spades). This project just goes to hell that little bit further every day; I wonder how long until the articles I've written are bastardised in the way all of these are?  talk \"\n",
      "######################################\n",
      "jd2718 sock fiend hes lying ass ani page i cannot comment im m1rth person im\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "jd2718 the sock fiend \n",
      "\n",
      "now hes lying out his ass on the ani page but I can't comment. and no im not this m1rth person, im just me.\n",
      "######################################\n",
      "i learned suck people good fisherqueen grammer fairly obvious typos pick a mistake bitch\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "I just learned how to suck up to people. You're very good at it, FisherQueen. As for the grammer, it should be fairly obvious that they're typos, now pick out a mistake here, bitch!\n",
      "######################################\n",
      "ram ur ass hard till ur eyes water\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ram it up ur ass very hard till ur eyes water\n",
      "######################################\n",
      "ms jackson lyrics andre yeah one right goes baby s mamas mamas mamas mamas baby mamas mamas yeah go like chorus andre i sorry ms jackson oooh i real never meant make daughter cry i apologize a trillion times i sorry ms jackson oooh i real never meant make daughter cry i apologize a trillion times big boi baby s drama mama like doin things like havin boys come neighborhood studio tryin fight need get a piece american pie take bite house i disconnect cable turn lights let know grandchild a baby a paycheck private school daycare shit medical bills i pay i love mom everything see i t one laid wanna rib start a custody war lawyers stay never got a chance hear side story divided fish fries cookouts child s birthday i t invited despite i show utmost respect i fall defend lady i call yeah chorus i sorry ms jackson oooh i real never meant make daughter cry i apologize a trillion times i sorry ms jackson oooh i real never meant make daughter cry i apologize a trillion times andre daughter got a special thang goin say puppy love arf arf arf say full grown arf arf woof hope feel feel way forever plan a pretty picnic cannot predict weather ms jackson ten times nine i lyin fine quickest muzzle throw mouth i decline king meets queen puppy love thing together dream bout crib goodyear swing oak tree i hope feel like forever forever forever ever forever ever forever never seems long grown notice day day ruler cannot long ms jackson intentions good i wish i could become a magician abracadabra sadder thoughts thoughts thoughts askin happened feelin i pray much need knee pads happened a reason one cannot mad know know everything cool yes i present first day school graduation chorus i sorry ms jackson oooh i real never meant make daughter cry i apologize a trillion times i sorry ms jackson oooh i real never meant make daughter cry i apologize a trillion times big boi uh uh yeah look way treats shit look way treat see little nosy ass homegirls done got ass sent creek g without a paddle left straddle ride thing girl t speakin cause dick mouth know i talking jealousy infidelity envy cheating beating d g thing placin blame keep singin song let bygones bygones go get hell mama chorus i sorry ms jackson oooh i real never meant make daughter cry i apologize a trillion times i sorry ms jackson oooh i real never meant make daughter cry i apologize a trillion times i sorry ms jackson oooh i real never meant make daughter cry i apologize a trillion times i sorry ms jackson oooh i real never meant make daughter cry i apologize a trillion times\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "\"\n",
      "\n",
      " MS JACKSON LYRICS \n",
      "\n",
      "[Andre 3000]\n",
      "Yeah this one right here goes out to all the baby's mamas, mamas...\n",
      "Mamas, mamas, baby mamas, mamas\n",
      "Yeah, go like this\n",
      "\n",
      "CHORUS: Andre 3000\n",
      "I'm sorry Ms. Jackson [OOOH]\n",
      "I am for real\n",
      "Never meant to make your daughter cry\n",
      "I apologize a trillion times\n",
      "I'm sorry Ms. Jackson [OOOH]\n",
      "I am for real\n",
      "Never meant to make your daughter cry\n",
      "I apologize a trillion times\n",
      "\n",
      "[Big Boi]\n",
      "My baby's drama mama, don't like me\n",
      "She be doin things like havin them boys come from her neighborhood \n",
      "to the studio tryin to fight me\n",
      "She need to get a, piece of the american pie and take her bite out\n",
      "That's my house, I'll disconnect the cable and turn the lights out\n",
      "And let her know her grandchild is a baby, and not a paycheck\n",
      "Private school, daycare shit, medical bills, I pay that\n",
      "I love your mom and everything, but see I ain't the one who laid down\n",
      "She wanna rib you up to start a custody war, my lawyers stay down\n",
      "She never got a chance \n",
      "to hear my side of the story we was divided\n",
      "She had fish fries and cookouts \n",
      "for my child's birthday I ain't invited\n",
      "Despite it, I show her the utmost respect when I fall through\n",
      "All you, do is defend that lady when I call you, yeah\n",
      "\n",
      "CHORUS:\n",
      "I'm sorry Ms. Jackson [OOOH]\n",
      "I am for real\n",
      "Never meant to make your daughter cry\n",
      "I apologize a trillion times\n",
      "I'm sorry Ms. Jackson [OOOH]\n",
      "I am for real\n",
      "Never meant to make your daughter cry\n",
      "I apologize a trillion times\n",
      "\n",
      "[Andre 3000]\n",
      "Me and your daughter.. got a special thang goin on\n",
      "You say it's puppy love (arf arf arf)\n",
      "We say it's full grown (ARF ARF WOOF!)\n",
      "Hope that we feel this.. feel this way forever\n",
      "You can plan a pretty picnic\n",
      "but you can't predict the weather, Ms. Jackson\n",
      "\n",
      "Ten times out of nine, now if I'm lyin; fine\n",
      "The quickest muzzle throw it on my mouth and I'll decline\n",
      "King meets queen, then the puppy love thing, together dream\n",
      "bout that crib with the Goodyear swing\n",
      "on the oak tree, I hope we feel like this forever\n",
      "Forever, forever, ever, forever, ever?\n",
      "Forever never seems that long until you're grown\n",
      "And notice that the day by day ruler can't be too long\n",
      "Ms. Jackson my intentions were good I wish I could\n",
      "become a magician to abracadabra all the sadder\n",
      "thoughts of me, thoughts of she, thoughts of he\n",
      "Askin what happened to the feelin that her and me\n",
      "Had, I pray so much about it need some knee, pads\n",
      "It happened for a reason one can't be, mad\n",
      "So know this, know that everything is cool\n",
      "And yes I will be present on the first day of school, \n",
      "and graduation\n",
      "\n",
      "CHORUS:\n",
      "I'm sorry Ms. Jackson [OOOH]\n",
      "I am for real\n",
      "Never meant to make your daughter cry\n",
      "I apologize a trillion times\n",
      "I'm sorry Ms. Jackson [OOOH]\n",
      "I am for real\n",
      "Never meant to make your daughter cry\n",
      "I apologize a trillion times\n",
      "\n",
      "[Big Boi]\n",
      "Uh, uh, yeah\n",
      "\"\"Look at the way he treats me\"\" \n",
      "Shit, look at the way you treat me\n",
      "You see your little nosy-ass homegirls \n",
      "done got your ass sent up the creek G\n",
      "Without a paddle, you left to straddle \n",
      "and ride this thing on out\n",
      "Now you and your girl ain't speakin no more \n",
      "cause my dick all in her mouth\n",
      "You know what I'm talking about Jealousy, infidelity, envy\n",
      "Cheating to beating, and D to the G they be the same thing\n",
      "So who you placin the blame on, you keep on singin the same song\n",
      "Let bygones be bygones, you can go on and get the hell on\n",
      "You and your mama\n",
      "\n",
      "CHORUS:\n",
      "I'm sorry Ms. Jackson [OOOH]\n",
      "I am for real\n",
      "Never meant to make your daughter cry\n",
      "I apologize a trillion times\n",
      "I'm sorry Ms. Jackson [OOOH]\n",
      "I am for real\n",
      "Never meant to make your daughter cry\n",
      "I apologize a trillion times\n",
      "I'm sorry Ms. Jackson [OOOH]\n",
      "I am for real\n",
      "Never meant to make your daughter cry\n",
      "I apologize a trillion times\n",
      "I'm sorry Ms. Jackson [OOOH]\n",
      "I am for real\n",
      "Never meant to make your daughter cry\n",
      "I apologize a trillion times\"\n",
      "######################################\n",
      "nice suck admins know get shut show says wp mos wherever wikipedia says episode table widths must wide show people x whatever screen resolution stating table width a problem i ask\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Nice suck up to the admins there... You know all you have to do to get me to shut up about this is show me where it says in the WP:MoS or wherever on Wikipedia that it says episode table widths must be 70% wide. Show me that, or where people with 1024 x whatever screen resolution are stating that this table width is a problem for them. That's all I ask.\n",
      "######################################\n",
      "dalit oppressed one insult ambedkar a key dalit emanciptor uses term dalit a badge honor insulting words low caste people would chamaar harijan dalit official word castelesss people indian law dalit a personal attack like african american hispanic american opposed nigger wetback\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "Dalit (the oppressed one) is not an insult. Ambedkar (a key Dalit emanciptor) uses the term 'Dalit' as a badge of honor. Insulting words for low caste people would be 'chamaar' or 'harijan'. Dalit is the official word for castelesss people in Indian Law. 'Dalit' is not a personal attack. It's like 'African-American' or 'Hispanic-American' as opposed to 'nigger' or 'wetback'.\n",
      "######################################\n",
      "chesdovi chesdovi a dos usa probably doesnt even speak hebrew think would dumb enough something obvious think really dumb one\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "chesdovi \n",
      "\n",
      "chesdovi is just a dos from the usa who probably doesnt even speak hebrew. if you think he would be dumb enough to do something so obvious and you think this is him, then you are really the dumb one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################\n"
     ]
    }
   ],
   "source": [
    "for x in range(20):\n",
    "    current = Error1.index[x]\n",
    "    print(Error1[current])\n",
    "    print('@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@')\n",
    "    print(X_train_origin[current])\n",
    "    print('######################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
